{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1679530887565,
     "user": {
      "displayName": "Wayne",
      "userId": "11546361323044434145"
     },
     "user_tz": -480
    },
    "id": "A9KePDZ41qls",
    "outputId": "7948aca7-9957-4fd6-e1f0-6e41af4d2fc5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !unzip -o 产品评论观点提取-new.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14884,
     "status": "ok",
     "timestamp": 1679531686388,
     "user": {
      "displayName": "Wayne",
      "userId": "11546361323044434145"
     },
     "user_tz": -480
    },
    "id": "Qn1LS36G41eJ",
    "outputId": "42808a11-a3da-45cd-9fa2-18aa8147eb69",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import ConcatDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# file_name='data/train_data_public.csv'\n",
    "# data = pd.read_csv(file_name,index_col=0)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     13,
     146,
     173
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert data to ids: 100%|███████████████████████████████████████████████████████| 7528/7528 [00:06<00:00, 1152.48it/s]\n",
      "Convert data to ids: 100%|███████████████████████████████████████████████████████| 2883/2883 [00:02<00:00, 1324.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "train_path = 'data/train_data_public.csv'\n",
    "test_path = 'data/test_public.csv'\n",
    "model_name = 'bert-base-chinese'\n",
    "\n",
    "max_len = 100\n",
    "batch_size = 16\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    do_lower_case=True) \n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, max_len, mode):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        self.max_len = max_len\n",
    "        self.label_dict = self.get_labels()\n",
    "        self.label_number = len(self.label_dict)\n",
    "        self.data_set = self.convert_data_to_ids(file_path)\n",
    "    \n",
    "    def _read(self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        samples = [] #dictionary-like list for recording original data sample\n",
    "        for idx,row in df.iterrows():\n",
    "            text = row['text']\n",
    "            if(type(text)==float):\n",
    "                print(text)\n",
    "                continue\n",
    "            tokens = list(row['text'])\n",
    "            if(self.mode == 'test'):\n",
    "                tags = []\n",
    "                class_ = None\n",
    "            else:\n",
    "                tags = row['BIO_anno'].split()\n",
    "                class_ = row['class']\n",
    "            samples.append({\"tokens\": tokens, \"labels\":tags, \"class\":class_})\n",
    "        return samples\n",
    "\n",
    "    # make a label dictionary: key=string, value=index(id)\n",
    "    def get_labels(self):\n",
    "        label_dic={}\n",
    "        label_list=[\"B-BANK\",\"I-BANK\",\"B-PRODUCT\",\"O\",\"I-PRODUCT\",\"B-COMMENTS_N\",\"I-COMMENTS_N\",\"B-COMMENTS_ADJ\",\"I-COMMENTS_ADJ\"]\n",
    "        for idx,label in enumerate(label_list):\n",
    "            label_dic[label]=idx\n",
    "\n",
    "        return label_dic\n",
    "\n",
    "    def convert_data_to_ids(self, file_path):\n",
    "        self.data_set=[]\n",
    "        samples=self._read(file_path)\n",
    "        for sample in tqdm(samples, desc=\"Convert data to ids\", disable=False):\n",
    "            if self.mode == 'train':\n",
    "                sample = self.convert_sample_to_id_train(sample) \n",
    "            else :\n",
    "                sample = self.convert_sample_to_id_test(sample)\n",
    "                \n",
    "            self.data_set.append(sample)\n",
    "        return self.data_set\n",
    "\n",
    "    def convert_sample_to_id_train(self, sample):\n",
    "        # adding more details to a single sample\n",
    "        # 1. tokens -> input_ids && token_type_ids\n",
    "        # 2. labels -> labels_ids\n",
    "        # 3. class is useless\n",
    "        # AuxInfo: attention_mask/position_ids/len\n",
    "        \n",
    "        tokens = sample[\"tokens\"]\n",
    "        labels = sample[\"labels\"]\n",
    "        class_ = sample[\"class\"]\n",
    "        assert len(tokens) == len(labels), 'unmatched things happen'\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if not len(self.tokenizer.tokenize(token)):\n",
    "                new_tokens.append('[UNK]')\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        if len(new_tokens) > self.max_len - 2:\n",
    "            new_tokens = new_tokens[:self.max_len - 2]\n",
    "            labels = labels[:self.max_len - 2]\n",
    "\n",
    "        new_tokens = [\"[CLS]\"] + new_tokens + [\"[SEP]\"]\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        # the additional two 'O' correspond to '[CLS]' and '[SEP]'\n",
    "        labels_ids = [self.label_dict[\"O\"]] + [self.label_dict[l] for l in labels] + [self.label_dict[\"O\"]]\n",
    "        # fill some shorter sample to the max_len with nonsense [PAD]\n",
    "        padding_id = self.tokenizer.convert_tokens_to_ids(['[PAD]'])\n",
    "        len_ = len(input_ids)\n",
    "\n",
    "        input_ids = input_ids + padding_id * (self.max_len - len(input_ids))\n",
    "        attention_mask = attention_mask + [0] * (self.max_len - len(attention_mask))\n",
    "        labels_ids = labels_ids + [self.label_dict[\"O\"]] * (self.max_len -len(labels_ids))\n",
    "        token_type_ids = [0] * len(input_ids) #?? for what?\n",
    "        position_ids = list(np.arange(len(input_ids)))\n",
    "        sample[\"input_ids\"] = input_ids\n",
    "        sample[\"labels_ids\"] = labels_ids\n",
    "        sample[\"attention_mask\"] = attention_mask\n",
    "        sample[\"token_type_ids\"] = token_type_ids\n",
    "        sample[\"position_ids\"] = position_ids\n",
    "        sample[\"class\"] = class_\n",
    "        sample[\"len\"] = len_\n",
    "        assert len(input_ids) == len(labels_ids), \"input unmatch with label-length\"        \n",
    "        assert len(input_ids) == self.max_len\n",
    "        return sample\n",
    "\n",
    "    def convert_sample_to_id_test(self, sample):\n",
    "        tokens = sample[\"tokens\"]\n",
    "        \n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if not len(self.tokenizer.tokenize(token)):\n",
    "                new_tokens.append('[UNK]')\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        if len(new_tokens) > self.max_len - 2:\n",
    "            new_tokens = new_tokens[:self.max_len - 2]\n",
    "\n",
    "        new_tokens = [\"[CLS]\"] + new_tokens + [\"[SEP]\"]\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_id = self.tokenizer.convert_tokens_to_ids(['[PAD]'])\n",
    "        len_ = len(input_ids)\n",
    "\n",
    "        input_ids = input_ids + padding_id * (self.max_len - len(input_ids))\n",
    "        attention_mask = attention_mask + [0] * (self.max_len - len(attention_mask))\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "        position_ids = list(np.arange(len(input_ids)))\n",
    "        sample[\"input_ids\"] = input_ids\n",
    "        sample[\"attention_mask\"] = attention_mask\n",
    "        sample[\"token_type_ids\"] = token_type_ids\n",
    "        sample[\"position_ids\"] = position_ids\n",
    "        sample[\"len\"] = len_\n",
    "        assert len(input_ids) == self.max_len\n",
    "        return sample\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_set)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.data_set[idx]\n",
    "        return instance\n",
    "\n",
    "# collate_func_x: organize the Dataset into a dictionary combination\n",
    "def collate_func_train(batch_data):\n",
    "    batch_size = len(batch_data)\n",
    "    if batch_size == 0:\n",
    "        return{}\n",
    "    input_ids_list, attention_mask_list, token_type_ids_list, labels_ids_list = [], [], [], []\n",
    "    position_ids_list, tokens_list = [], []\n",
    "    len_list = []\n",
    "    class_list = []\n",
    "    for instance in batch_data:\n",
    "        input_ids_list.append(instance[\"input_ids\"])\n",
    "        attention_mask_list.append(instance[\"attention_mask\"])\n",
    "        token_type_ids_list.append(instance[\"token_type_ids\"])\n",
    "        labels_ids_list.append(instance[\"labels_ids\"])\n",
    "        position_ids_list.append(instance[\"position_ids\"])\n",
    "        tokens_list.append(instance[\"tokens\"])\n",
    "        len_list.append(instance[\"len\"])\n",
    "        class_list.append(instance[\"class\"])\n",
    "    \n",
    "    return {\"input_ids\": torch.tensor(input_ids_list, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask_list, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids_list, dtype=torch.long),\n",
    "            \"position_ids\": torch.tensor(position_ids_list, dtype=torch.long),\n",
    "            \"labels_ids\": torch.tensor(labels_ids_list, dtype=torch.long),\n",
    "            \"classes\": torch.tensor(class_list, dtype=torch.long),\n",
    "            \"tokens\": tokens_list,\n",
    "            \"lens\": len_list}\n",
    "\n",
    "def collate_func_test(batch_data):\n",
    "    batch_size = len(batch_data)\n",
    "    if batch_size == 0:\n",
    "        return{}\n",
    "    input_ids_list, attention_mask_list, token_type_ids_list = [], [], []\n",
    "    position_ids_list, tokens_list = [], []\n",
    "    len_list = []\n",
    "    for instance in batch_data:\n",
    "        input_ids_list.append(instance[\"input_ids\"])\n",
    "        attention_mask_list.append(instance[\"attention_mask\"])\n",
    "        token_type_ids_list.append(instance[\"token_type_ids\"])\n",
    "        position_ids_list.append(instance[\"position_ids\"])\n",
    "        tokens_list.append(instance[\"tokens\"])\n",
    "        len_list.append(instance[\"len\"])\n",
    "    \n",
    "    return {\"input_ids\": torch.tensor(input_ids_list, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask_list, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids_list, dtype=torch.long),\n",
    "            \"position_ids\": torch.tensor(position_ids_list, dtype=torch.long),\n",
    "            \"tokens\": tokens_list,\n",
    "            \"len\": torch.tensor(len_list, dtype=torch.long)}\n",
    "\n",
    "train_data_original = MyDataSet(tokenizer, train_path, max_len, mode='train')\n",
    "test_data_original = MyDataSet(tokenizer, test_path, max_len, mode='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "modal_name = \"bert-base-chinese\"\n",
    "hidden_size = 768 # the output size of BERT\n",
    "num_label = len(train_data_original.label_dict)\n",
    "num_classes=3\n",
    "\n",
    "class BERTLinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTLinearModel, self).__init__()\n",
    "    \n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.classifier4NER = nn.Linear(hidden_size, num_label)\n",
    "        self.classifier4SA = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, device, batch):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        position_ids = batch[\"position_ids\"].to(device)\n",
    "        bert_output = self.bert(input_ids, attention_mask=attention_mask, \n",
    "                      token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "        \n",
    "        sequence_output, pooled_output = bert_output[0], bert_output[1]\n",
    "    \n",
    "        ner_logits = self.classifier4NER(sequence_output)\n",
    "        sa_logits = self.classifier4SA(pooled_output)\n",
    "        out = ner_logits, sa_logits\n",
    "        return out\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X):\n",
    "    assert k > 1\n",
    "    fold_size = len(X) // k  \n",
    "    \n",
    "    X_train = None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)  #slice(start,end,step):return a 'slice' object\n",
    "        X_part = X[idx]\n",
    "        if j == i: # take j_th fold as valid\n",
    "            X_valid = X_part\n",
    "        elif X_train is None:\n",
    "            X_train = X_part\n",
    "        else:\n",
    "            X_train = ConcatDataset([X_train, X_part])\n",
    "    return X_train,  X_valid\n",
    " \n",
    "\n",
    "def k_fold(model, k, train_data_original, num_epochs=3,learning_rate=0.001, batch_size=5):\n",
    "    train_loss_sum, valid_loss_sum = 0, 0\n",
    "    train_acc_sum ,valid_acc_sum = 0,0\n",
    "\n",
    "    for i in range(k):\n",
    "        # get the train_data and valid_data from original train_data\n",
    "        train_data, val_data = get_k_fold_data(k, i, train_data_original) \n",
    "        \n",
    "        train_ls, valid_ls = train(model, train_data, val_data, num_epochs, learning_rate, batch_size)\n",
    "        \n",
    "        # regard the last epoch's result as this train's final result\n",
    "        print(\n",
    "            f'''Fold: {i + 1}\n",
    "          | Train Loss: {train_ls[-1][0]: .3f}\n",
    "          | Train Accuracy: {train_ls[-1][1]: .3f}\n",
    "          | Val Loss: {valid_ls[-1][0]: .3f}\n",
    "          | Val Accuracy: {valid_ls[-1][1]: .3f}''')\n",
    "        \n",
    "        train_loss_sum += train_ls[-1][0]\n",
    "        valid_loss_sum += valid_ls[-1][0]\n",
    "        train_acc_sum += train_ls[-1][1]\n",
    "        valid_acc_sum += valid_ls[-1][1]\n",
    "        \n",
    "    print(\n",
    "            f'''Finally Result: \n",
    "          | train_loss_sum: {train_loss_sum/k: .3f}\n",
    "          | train_acc_sum: {train_acc_sum/k: .3f}\n",
    "          | valid_loss_sum: {valid_loss_sum/k: .3f}\n",
    "          | valid_acc_sum: {valid_acc_sum/k: .3f}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "D:\\CodeField\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Iter_train (loss=0.458): 100%|████████████████████████████████████| 423/423 [01:53<00:00,  3.73it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.31it/s]\n",
      "Iter_train (loss=0.213): 100%|████████████████████████████████████| 423/423 [01:52<00:00,  3.75it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "          | Train Loss:  0.019\n",
      "          | Train Accuracy:  98.531\n",
      "          | Val Loss:  0.026\n",
      "          | Val Accuracy:  98.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter_train (loss=0.467): 100%|████████████████████████████████████| 423/423 [01:53<00:00,  3.73it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.16it/s]\n",
      "Iter_train (loss=0.095): 100%|████████████████████████████████████| 423/423 [01:53<00:00,  3.71it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n",
      "          | Train Loss:  0.011\n",
      "          | Train Accuracy:  99.134\n",
      "          | Val Loss:  0.009\n",
      "          | Val Accuracy:  99.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter_train (loss=0.140): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.70it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.10it/s]\n",
      "Iter_train (loss=0.068): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.70it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n",
      "          | Train Loss:  0.006\n",
      "          | Train Accuracy:  99.419\n",
      "          | Val Loss:  0.006\n",
      "          | Val Accuracy:  99.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter_train (loss=0.125): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.69it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.06it/s]\n",
      "Iter_train (loss=0.034): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.69it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n",
      "          | Train Loss:  0.005\n",
      "          | Train Accuracy:  99.555\n",
      "          | Val Loss:  0.004\n",
      "          | Val Accuracy:  99.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter_train (loss=0.027): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.69it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.03it/s]\n",
      "Iter_train (loss=0.040): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.69it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n",
      "          | Train Loss:  0.004\n",
      "          | Train Accuracy:  99.662\n",
      "          | Val Loss:  0.002\n",
      "          | Val Accuracy:  99.890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter_train (loss=0.043): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.68it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.02it/s]\n",
      "Iter_train (loss=0.019): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.68it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 6\n",
      "          | Train Loss:  0.003\n",
      "          | Train Accuracy:  99.745\n",
      "          | Val Loss:  0.002\n",
      "          | Val Accuracy:  99.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter_train (loss=0.038): 100%|████████████████████████████████████| 423/423 [01:55<00:00,  3.68it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.00it/s]\n",
      "Iter_train (loss=0.022): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.68it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n",
      "          | Train Loss:  0.003\n",
      "          | Train Accuracy:  99.829\n",
      "          | Val Loss:  0.003\n",
      "          | Val Accuracy:  100.198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter_train (loss=0.027): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.68it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.01it/s]\n",
      "Iter_train (loss=0.035): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.68it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n",
      "          | Train Loss:  0.003\n",
      "          | Train Accuracy:  99.956\n",
      "          | Val Loss:  0.003\n",
      "          | Val Accuracy:  99.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter_train (loss=0.037): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.68it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.03it/s]\n",
      "Iter_train (loss=0.037): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.68it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9\n",
      "          | Train Loss:  0.003\n",
      "          | Train Accuracy:  100.079\n",
      "          | Val Loss:  0.004\n",
      "          | Val Accuracy:  99.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter_train (loss=0.051): 100%|████████████████████████████████████| 423/423 [01:54<00:00,  3.68it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.01it/s]\n",
      "Iter_train (loss=0.014): 100%|████████████████████████████████████| 423/423 [01:55<00:00,  3.65it/s]\n",
      "Iter_valid: 100%|███████████████████████████████████████████████████| 47/47 [00:03<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 10\n",
      "          | Train Loss:  0.002\n",
      "          | Train Accuracy:  100.054\n",
      "          | Val Loss:  0.001\n",
      "          | Val Accuracy:  100.311\n",
      "Finally Result: \n",
      "          | train_loss_sum:  0.006\n",
      "          | train_acc_sum:  99.596\n",
      "          | valid_loss_sum:  0.006\n",
      "          | valid_acc_sum:  99.720\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "label_dict = train_data_original.label_dict\n",
    "id2dict = {v: k for k, v in label_dict.items()} # reverse the dict_map: from k:v to v:k\n",
    "\n",
    "def train(model, train_data, val_data, num_epochs, learning_rate, batch_size):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_loader = DataLoader(dataset=train_data,\n",
    "              batch_size = batch_size,\n",
    "              collate_fn = collate_func_train,\n",
    "              shuffle = True)\n",
    "    val_loader = DataLoader(dataset=val_data, \n",
    "                batch_size = batch_size, \n",
    "                collate_fn=collate_func_train)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.to(device)\n",
    "    train_ls, val_ls = [], []\n",
    "    for i_epoch in range(num_epochs):\n",
    "        # ------ Train -----------\n",
    "        model.train()\n",
    "        total_acc_train, total_loss_train = 0, 0\n",
    "        \n",
    "        # try-except structure is used for 'tqdm' bug\n",
    "        try:\n",
    "            with tqdm(train_loader, desc=\"Iter_train:\", ncols=100) as t:\n",
    "                for batch in t:\n",
    "                    labels_ids = batch[\"labels_ids\"].to(device)\n",
    "                    classes = batch[\"classes\"].to(device)\n",
    "                    ner_logit, sa_logit = model(device, batch)\n",
    "                    \n",
    "                    # calculate loss\n",
    "                    ner_loss = loss_fct(ner_logit.view(-1,num_label), labels_ids.view(-1))\n",
    "                    sa_loss = loss_fct(sa_logit, classes) #classes有点怪，跟num_label对不上\n",
    "                    loss = ner_loss + sa_loss # for backward()\n",
    "                    total_loss_train += loss.item()\n",
    "                    \n",
    "                    # calculate accuracy\n",
    "                    ner_acc = (ner_logit.argmax(dim=-1) == labels_ids).sum()\n",
    "                    sa_acc = (sa_logit.argmax(dim=-1) == classes).sum()\n",
    "                    acc = ner_acc + sa_acc\n",
    "                    total_acc_train += acc.item()\n",
    "\n",
    "                    # model update \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # update tqdm's progress bar using new loss value\n",
    "                    t.set_description(\"Iter_train (loss=%5.3f)\" % loss.item()) # .item(): higher precision\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            t.close()\n",
    "            raise\n",
    "        t.close()\n",
    "\n",
    "        \n",
    "        # ------ Valid -----------\n",
    "        # switch th eval mode for valid dataset\n",
    "        model.eval()\n",
    "        total_acc_val, total_loss_val = 0, 0\n",
    "\n",
    "        try:\n",
    "            with tqdm(val_loader, desc=\"Iter_valid\", ncols=100) as t:\n",
    "                for batch in t:\n",
    "                    labels_ids = batch[\"labels_ids\"].to(device)\n",
    "                    classes = batch[\"classes\"].to(device)\n",
    "                    # no backward(), so no autograd, which will consume memory\n",
    "                    with torch.no_grad():\n",
    "                        ner_logit, sa_logit = model(device, batch)\n",
    "\n",
    "                    ner_loss = loss_fct(ner_logit.view(-1,num_label), labels_ids.view(-1))\n",
    "                    sa_loss = loss_fct(sa_logit, classes) \n",
    "                    loss = ner_loss + sa_loss \n",
    "                    total_loss_val += loss.item()\n",
    "\n",
    "                    ner_acc = (ner_logit.argmax(dim=-1) == labels_ids).sum()\n",
    "                    sa_acc = (sa_logit.argmax(dim=-1) == classes).sum()\n",
    "                    acc = ner_acc + sa_acc\n",
    "                    total_acc_val += acc.item()\n",
    "        except KeyboardInterrupt:\n",
    "            t.close()\n",
    "            raise\n",
    "        t.close()\n",
    "        \n",
    "        train_loss_rate = total_loss_train / len(train_data)\n",
    "        train_acc_rate = total_acc_train / len(train_data)\n",
    "        val_loss_rate = total_loss_val / len(val_data)\n",
    "        val_acc_rate = total_acc_val / len(val_data)\n",
    "\n",
    "        train_ls.append((train_loss_rate, train_acc_rate))\n",
    "        val_ls.append((val_loss_rate, val_acc_rate))\n",
    "        \n",
    "    # return all epoch's results\n",
    "    return train_ls, val_ls\n",
    "\n",
    "    \n",
    "model = BERTLinearModel()\n",
    "k_fold(model=model, k=10,train_data_original=train_data_original,\n",
    "       num_epochs = 2, learning_rate = 1e-5, batch_size = 16)\n",
    "\n",
    "#sava train parameters\n",
    "torch.save(model.state_dict(), 'model.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter_predict: 100%|███████████████████████████████████████████████| 181/181 [00:15<00:00, 11.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>BIO_anno</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>O O O O O O O O O B-BANK I-BANK O O O B-COMMEN...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>O O O O O O O O O B-PRODUCT I-PRODUCT B-COMMEN...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>O O O B-PRODUCT I-PRODUCT O O O O O O O O O O ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>O O O O O O B-BANK I-BANK O O O O O O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>O O O O O O O O O O O O O O O B-COMMENTS_N I-C...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>O O O O O O O O O O B-BANK I-BANK O O O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           BIO_anno  class\n",
       "0   0  O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O ...      2\n",
       "1   1  O O O O O O O O O B-BANK I-BANK O O O B-COMMEN...      0\n",
       "2   2  O O O O O O O O O B-PRODUCT I-PRODUCT B-COMMEN...      2\n",
       "3   3  O O O B-PRODUCT I-PRODUCT O O O O O O O O O O ...      2\n",
       "4   4              O O O O O O B-BANK I-BANK O O O O O O      2\n",
       "5   5  O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ ...      2\n",
       "6   6  O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O ...      2\n",
       "7   7  O O O O O O O O O O O O O O O B-COMMENTS_N I-C...      2\n",
       "8   8  O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_...      2\n",
       "9   9            O O O O O O O O O O B-BANK I-BANK O O O      2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    label_dict = test_data_original.label_dict\n",
    "    id2dict = {v: k for k, v in label_dict.items()} # reverse the dict_map: from k:v to v:k\n",
    "    test_loader = DataLoader(test_data_original,\n",
    "                             batch_size = batch_size, \n",
    "                             collate_fn=collate_func_test)\n",
    "    \n",
    "    ner_predict = []\n",
    "    sa_predict = []\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "    try:\n",
    "        with tqdm(test_loader, desc=\"Iter_predict\", ncols=100) as t:\n",
    "            for batch in t:\n",
    "                len_list = batch[\"len\"].to(device)\n",
    "                with torch.no_grad():\n",
    "                    ner_logit, sa_logit = model(device, batch)\n",
    "                ner = torch.argmax(ner_logit, dim=-1).cpu().numpy().tolist()\n",
    "                sa = torch.argmax(sa_logit, dim=-1).cpu().numpy().tolist()\n",
    "                \n",
    "                for idy in range(len(ner)):\n",
    "                    ner_seq = ner[idy][1:len_list[idy]+1] # remove [CLS] and [PAD] etc. in terms of the 'len_list'\n",
    "                    ner_res = [id2dict[idx] for idx in ner_seq]\n",
    "                    ner_predict.append(' '.join(ner_res))\n",
    "                    \n",
    "                sa_predict.extend(sa)\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        t.close()\n",
    "        raise\n",
    "    t.close()\n",
    "    \n",
    "    return ner_predict, sa_predict\n",
    "\n",
    "# load parameters of previous saved model\n",
    "model.load_state_dict(torch.load('model.pth')) \n",
    "ner_predict, sa_predict = predict(model)\n",
    "\n",
    "result_data=[]\n",
    "for idx,(bio,cls) in enumerate(zip(ner_predict, sa_predict)):\n",
    "    result_data.append([idx,bio,cls])\n",
    "\n",
    "submit=pd.DataFrame(result_data,columns=['id','BIO_anno','class'])\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "submit.head(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPKIKvtyYtT+fEsRw8AHcjO",
   "mount_file_id": "1iiGeN9y3VbMFk5OkYV2BRNSGzlQrsAQ9",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}